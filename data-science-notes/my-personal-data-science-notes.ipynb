{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 align=\"center\"  style='color:blue;font-size:30px'> My Personal Data Science Notes </h1>\n<h2> üëã Hi Everyone, </h2>\n\nHere are my personal notes about various **fundamental** and **unique** methods or functions related to **Data Science** that are gathered from multiple notebooks and sources (Mostly from a <a href=\"https://www.coursera.org/learn/competitive-data-science/\">Coursera Course</a>). \n\nI wrote this notebook to support my journey in learning Data Science and I hope that this notebook would be useful for fellow learners too. Thank you!\n\n<a id=\"toc\"></a>\n<h2>üó®Ô∏è List of contents:</h2>\n<div style=\"background: #f9f9f9 none repeat scroll 0 0;border: 1px solid #aaa;display: table;font-size: 95%;margin-bottom: 1em;padding: 20px;width: 400px;\">\n<ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">\n    <li style=\"list-style: outside none none !important;\"><a href=\"#common\">Common Library and Utilities</a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#tod\">Data Types & Preprocessing</a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#eda\">Exploratory Data Analysis</a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#validation\">Validation</a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#leakages\">Data Leakages</a>\n</ul>\n</div>"},{"metadata":{},"cell_type":"markdown","source":"# Common Library and Utilities <a id=\"common\"></a>\n\n<a href=\"#toc\"><img src= \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Circle-icons-arrow-up.svg/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left;margin-right:10px;\" >  Back to the list of contents</a>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Data wrangling\nimport pandas as pd\nimport numpy as np\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\n\n# OS\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm.notebook import tqdm # Progress bar\n\n# Scalers\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import shuffle\n\n# Models\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn.linear_model import Perceptron\nfrom sklearn import svm #support vector Machine\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.naive_bayes import GaussianNB #Naive bayes\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n# Cross-validation\nfrom sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nfrom sklearn.model_selection import cross_validate\n\n# GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n# Boost\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cat\n\n# Deep learning\nimport tensorflow as tf\nimport keras\nimport torch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)                # type: ignore\n    #torch.backends.cudnn.deterministic = True  # type: ignore\n    #torch.backends.cudnn.benchmark = True      # type: ignore\n    \n\n@contextmanager\ndef timer(name: str) -> None:\n    \"\"\"Timer Util\"\"\"\n    t0 = time.time()\n    print(\"[{}] start\".format(name))\n    yield\n    print(\"[{}] done in {:.0f} s\".format(name, time.time() - t0))\n\nset_seed(42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Types & Preprocessing <a id=\"tod\"></a>\n\n<a href=\"#toc\"><img src= \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Circle-icons-arrow-up.svg/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left;margin-right:10px;\" >  Back to the list of contents</a>"},{"metadata":{},"cell_type":"markdown","source":"### Numeric features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1. Scaling\n# MinMaxScaler : To [0,1]\nfrom sklearn.preprocessing import MinMaxScaler\n\n# StandardScaler : Mean=0, std=1\nfrom sklearn.preprocessing import StandardScaler\n\n# 2. Outliers\n# Winsorization : The main purpose of winsorization is to remove outliers by clipping feature's values.\n\n# 3. Rank\nfrom scipy.stats import rankdata\n\n# 4. Transformation\n# Log transform : np.log(1+x)\n# Raising to the power < 1 : np.sqrt(x + 2/3)\n\n# FEATURE GENERATION\n# Ex : Generating decimal feature of a sale","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ordinal features\nExample:\nTicket class, driver's license type, education"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label encoding\n# Alphabetical (sorted) : [S,C,Q]->[2,1,3]\nfrom sklearn.preprocessing import LabelEncoder\n\n# Order of appereance : [S,C,Q]->[1,2,3]\npandas.factorize","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# One-hot encoding\npandas.get_dummies\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Combine more two/more cat features to one features\n# Example: pclass + sex = pclass_sex","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Datetime features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Format example: 25.01.2009\ndf['date'] = pd.to_datetime(df['date'])\ndf['year'] = pd.DatetimeIndex(pd.to_datetime(df['date'], format='%d.%m.%Y')).year\ndf['month'] = pd.DatetimeIndex(pd.to_datetime(df['date'], format='%d.%m.%Y')).month\ndf['day'] = pd.DatetimeIndex(pd.to_datetime(df['date'], format='%d.%m.%Y')).day","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Additional external sources\n* <a href=\"https://scikit-learn.org/stable/modules/preprocessing.html\">Preprocessing in Sklearn</a> <br>\n* <a href=\"https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/\">Discover Feature Engineering</a>\n"},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA) <a id=\"eda\"></a>\n\n<a href=\"#toc\"><img src= \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Circle-icons-arrow-up.svg/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left;margin-right:10px;\" >  Back to the list of contents</a>"},{"metadata":{},"cell_type":"markdown","source":"### Transpose viewing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Useful to show all the columns\ntrain.head().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Finding out feature importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# Create a copy to work with\nX = train.copy()\n\n# Save and drop labels\ny = train.y\nX = X.drop('y', axis=1)\n\n# fill NANs \nX = X.fillna(-999)\n\n# Label encoder\nfor c in train.columns[train.dtypes == 'object']:\n    X[c] = X[c].factorize()[0]\n    \nrf = RandomForestClassifier()\nrf.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(rf.feature_importances_)\nplt.xticks(np.arange(X.shape[1]), X.columns.tolist(), rotation=90);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature encoding using factorize"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_enc =  pd.DataFrame(index = train.index)\n\nfor col in tqdm_notebook(traintest.columns):\n    train_enc[col] = train[col].factorize()[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using mask in .loc"},{"metadata":{"trusted":true},"cell_type":"code","source":"mask = (nunique.astype(float)/train.shape[0] < 0.8) & (nunique.astype(float)/train.shape[0] > 0.4)\ntrain.loc[:25, mask]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Retrieving categorical & numerical columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = list(train.select_dtypes(include=['object']).columns)\nnum_cols = list(train.select_dtypes(exclude=['object']).columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Functions"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"def autolabel(arrayA):\n    ''' Label each colored square with the corresponding data value. \n    If value > 20, the text is in black, else in white.\n    '''\n    arrayA = np.array(arrayA)\n    for i in range(arrayA.shape[0]):\n        for j in range(arrayA.shape[1]):\n                plt.text(j,i, \"%.2f\"%arrayA[i,j], ha='center', va='bottom',color='w')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def hist_it(feat):\n    ''' Make a histogram\n    '''\n    plt.figure(figsize=(16,4))\n    feat[Y==0].hist(bins=range(int(feat.min()),int(feat.max()+2)),normed=True,alpha=0.8)\n    feat[Y==1].hist(bins=range(int(feat.min()),int(feat.max()+2)),normed=True,alpha=0.5)\n    plt.ylim((0,1))\n    \ndef hist_it1(feat):\n    plt.figure(figsize=(16,4))\n    feat[Y==0].hist(bins=100,range=(feat.min(),feat.max()),normed=True,alpha=0.5)\n    feat[Y==1].hist(bins=100,range=(feat.min(),feat.max()),normed=True,alpha=0.5)\n    plt.ylim((0,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gt_matrix(feats,sz=16):\n    '''Make a > (greater than) matrix to observe patterns in features\n    '''\n    a = []\n    for i,c1 in enumerate(feats):\n        b = [] \n        for j,c2 in enumerate(feats):\n            mask = (~train[c1].isnull()) & (~train[c2].isnull())\n            if i>=j:\n                b.append((train.loc[mask,c1].values>=train.loc[mask,c2].values).mean())\n            else:\n                b.append((train.loc[mask,c1].values>train.loc[mask,c2].values).mean())\n\n        a.append(b)\n\n    plt.figure(figsize = (sz,sz))\n    plt.imshow(a, interpolation = 'None')\n    _ = plt.xticks(range(len(feats)),feats,rotation = 90)\n    _ = plt.yticks(range(len(feats)),feats,rotation = 0)\n    autolabel(a)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validation <a id=\"validation\"></a>\n\n<a href=\"#toc\"><img src= \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Circle-icons-arrow-up.svg/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left;margin-right:10px;\" >  Back to the list of contents</a>\n\n### Common validation strategies\n**Holdout scheme:**\n1. Split train data into two parts: partA and partB.\n2. Fit the model on partA, predict for partB.\n3. Use predictions for partB for estimating model quality. Find such hyper-parameters, that quality on partB is maximized.\n\n**K-Fold scheme:**\n1. Split train data into K folds.\n2. Iterate though each fold: retrain the model on all folds except current fold, predict for the current fold.\n3. Use the predictions to calculate quality on each fold. Find such hyper-parameters, that quality on each fold is maximized. You can also estimate mean and variance of the loss. This is very helpful in order to understand significance of improvement.\n\n**LOO (Leave-One-Out) scheme:**\n1. Iterate over samples: retrain the model on all samples except current sample, predict for the current sample. You will need to retrain the model N times (if N is the number of samples in the dataset).\n2. In the end you will get LOO predictions for every sample in the trainset and can calculate loss."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Holdout validation : n-groups = 1\n# Enough data but score and optimal parameter are similar\nfrom sklearn.model_selection import ShuffleSplit\n\n# K-fold : n-groups = k\n# Enough data but score and optimal parameter differ\nfrom sklearn.model_selection import Kfold\n\n# Leave-one-out : n-groups = len(train)\n# Small amount of data\nfrom sklearn.model_selection import LeaveOneOut","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Stratification** : Preserve the same target distribution over different folds\n\nStratification is useful for: <br>\n‚Ä¢ Small datasets <br>\n‚Ä¢ Unbalanced datasets <br>\n‚Ä¢ Multiclass clasification"},{"metadata":{},"cell_type":"markdown","source":"### Data splitting strategies\nValidation should be set up to mimic the train/test split of a competition\n\n1. Random, rowwise\n2. Timewise (Ex: Predicting sales in a shop)\n3. By ID\n4. Combined\n\n**Example:** <br>\n‚Ä¢ Train set: Ratio of Men >> Ratio of Women <br>\n‚Ä¢ Test set: Ratio of Women >> Ratio of Men\n\n**What to do ?** <br>\n*Adjust/force the validation data so that it mimic the test set (Ratio of Women >> Ratio of Men)*"},{"metadata":{},"cell_type":"markdown","source":"### Submission problems\n\nUsually on Kaggle it is allowed to select two final submissions, which will be checked against the private LB and contribute to the competitor's final position. A common practice is to select one submission with a **best validation score**, and another submission which **scored best on Public LB**.\n\n\n**Cause of LB Shuffle** <br>\n‚Ä¢ Randomness <br>\n‚Ä¢ Little amount of data <br>\n‚Ä¢ Different public/private distributions"},{"metadata":{},"cell_type":"markdown","source":"### Additional external sources\n* <a href=\"http://www.chioka.in/how-to-select-your-final-models-in-a-kaggle-competitio/\"> Advices on validation in a competition </a>\n* <a href=\"https://scikit-learn.org/stable/modules/cross_validation.html\"> Validation in Sklearn </a>\n"},{"metadata":{},"cell_type":"markdown","source":"# Data Leakages <a id=\"dl\"></a>\n\n<a href=\"#toc\"><img src= \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Circle-icons-arrow-up.svg/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left;margin-right:10px;\" >  Back to the list of contents</a>\n\nThe most common types of leaks:\n1. Date / Time\n2. Meta data\n3. Information in IDs -> Might be a hash of something\n4. Row order\n\n### Additional external source\n* <a href=\"https://www.kaggle.com/olegtrott/the-perfect-score-script\">The \"Perfect Score\" Script<a>"},{"metadata":{},"cell_type":"markdown","source":"<span style='color:blue;font-size:18px' >Work in progress...</span>"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}